# don't redraw X
x.sim <- x
y.sim <- 1 + b * x.sim + rnorm(n, sd = x / 3)
rec.sim[i] <- summary(lm(y.sim ~ x.sim))$coeff[2,1] - b
}
par(mfrow = c(1, 2))
### Bootstrapped sampling distribution in gray histogram
hist(rec.boot[,1], freq = F, col = "gray", breaks = 15, main = "Wild", xlab = expression(hat(b)))
### Simulated Sampling Distribution in red
lines(density(rec.sim), col = "red", lwd = 2)
### Model Based Sampling Distribution in Blue
lines(seq(-5, 5, by = .01), dnorm(seq(-5, 5, by = .01), mean = 0, sd = sqrt(1 / sum((x - mean(x))^2))), col = "blue", lwd = 2)
### Bootstrapped sampling distribution in gray histogram
hist(rec.boot[,2], freq = F, col = "gray", breaks = 15, main = "Pairs", xlab =  expression(hat(b)))
### Simulated Sampling Distribution in red
lines(density(rec.sim), col = "red", lwd = 2)
### Model Based Sampling Distribution in Blue
lines(seq(-5, 5, by = .01), dnorm(seq(-5, 5, by = .01), mean = 0, sd = sqrt(1 / sum((x - mean(x))^2))), col = "blue", lwd = 2)
# install.packages("lmboot")
library("lmboot")
library("lmtest")
library("sandwich")
fileName <- "https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/estate.csv"
housing_data <- read.csv(fileName)
names(housing_data)
mod <- lm(log(price) ~ log(area) + bed + bath + garage + quality,
data = housing_data)
summary(mod)
library(lme4)
fileName <- "https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/grace_plot_level.csv"
dat <- read.csv(fileName)
# Remove Missing Data
# Generally, we want to be careful about the data we remove
# As this may bias our estimates if the missingness is important
dat <- na.omit(dat)
dim(dat)
# Fit a linear model which disregards the site structure
mod <- lm(ln.prich~ ln.ptotmass + ln.pprod, data = dat)
summary(mod)
dat
plot(mod)
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
# hint: when feeding in the covariates, you will need to remove the first two columns
# from the bike_data_training set which correspond to the date and bike_counts
# so you will need to feed the function
x = as.matrix(bike_data_train[, - c(1,2)])
bike_data_train <- read.csv("https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/bike_data_18.csv")
dim(bike_data_train)
names(bike_data_train)
# hint: when feeding in the covariates, you will need to remove the first two columns
# from the bike_data_training set which correspond to the date and bike_counts
# so you will need to feed the function
x = as.matrix(bike_data_train[, - c(1,2)])
library(glmnet)
cv_lasso_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 1, family = "poisson")
plot(cv_lasso_fit)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.min)
# hint: when feeding in the covariates, you will need to remove the first two columns
# from the bike_data_training set which correspond to the date and bike_counts
# so you will need to feed the function
x = as.matrix(bike_data_train[, - c(1,2)])
library(glmnet)
cv_lasso_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 1, family = "poisson")
plot(cv_lasso_fit)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.min)
coef(cv_lasso_fit, s = lasso_mod$lambda.1se)
# hint: when feeding in the covariates, you will need to remove the first two columns
# from the bike_data_training set which correspond to the date and bike_counts
# so you will need to feed the function
x = as.matrix(bike_data_train[, - c(1,2)])
library(glmnet)
cv_lasso_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 1, family = "poisson")
plot(cv_lasso_fit)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.min)
coef(cv_lasso_fit, s = lasso_fit$lambda.1se)
# hint: when feeding in the covariates, you will need to remove the first two columns
# from the bike_data_training set which correspond to the date and bike_counts
# so you will need to feed the function
x = as.matrix(bike_data_train[, - c(1,2)])
library(glmnet)
cv_lasso_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 1, family = "poisson")
plot(cv_lasso_fit)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.min)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.1se)
cv_ridge_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 0, family = "posson")
cv_ridge_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 0, family = "poisson")
plot(cv_ridge_fit)
coef(cv_ridge_fit, s = cv_ridge_fit$lambda.min)
coef(cv_ridge_fit, s = cv_ridge_fit$lambda.1se)
# Test data from 2019
bike_data_test <- read.csv("https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/bike_data_19.csv")
# regular glm without any penalization
unpenalized_model <- glm(bike_counts ~ ., data = bike_data_train[, -1], family = "poisson")
# predictive accuracy for 2019 when using all covariates but no model seletion or penalization
## use type = "response" to get predictions in bikes, instead of log(bikes)
mean((bike_data_test$bike_counts - predict(unpenalized_model, newx = as.matrix(bike_data_test[,-c(1)]), type = "response"))^2)
## to get predictions for a penalized regression
## use type = "response" to get predictions in bikes, instead of log(bikes)
lasso_predicted <- predict(cv_lasso_fit, s = cv_lasso_fit$lambda.min, newx = x, type = "response")
lasso_test_error <- mean((bike_data_train$bike_counts - lasso_predicted)^2)
lasso_test_error
ridge_predicted <- predict(cv_ridge_fit, s = cv_ridge_fit$lambda.min, newx = x, type = "response")
ridge_test_error <- mean((bike_data_train$bike_counts - ridge_predicted)^2)
ridge_test_error
# hint: when feeding in the covariates, you will need to remove the first two columns
# from the bike_data_training set which correspond to the date and bike_counts
# so you will need to feed the function
x = as.matrix(bike_data_train[, - c(1,2)])
library(glmnet)
cv_lasso_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 1, family = "poisson")
plot(cv_lasso_fit)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.min)
coef(cv_lasso_fit, s = cv_lasso_fit$lambda.1se)
cv_ridge_fit <- cv.glmnet(x = x, y = bike_data_train$bike_counts, alpha = 0, family = "poisson")
plot(cv_ridge_fit)
coef(cv_ridge_fit, s = cv_ridge_fit$lambda.min)
coef(cv_ridge_fit, s = cv_ridge_fit$lambda.1se)
# Test data from 2019
bike_data_test <- read.csv("https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/bike_data_19.csv")
# regular glm without any penalization
unpenalized_model <- glm(bike_counts ~ ., data = bike_data_train[, -1], family = "poisson")
# predictive accuracy for 2019 when using all covariates but no model seletion or penalization
## use type = "response" to get predictions in bikes, instead of log(bikes)
mean((bike_data_test$bike_counts - predict(unpenalized_model, newx = as.matrix(bike_data_test[,-c(1)]), type = "response"))^2)
## to get predictions for a penalized regression
## use type = "response" to get predictions in bikes, instead of log(bikes)
lasso_predicted <- predict(cv_lasso_fit, s = cv_lasso_fit$lambda.min, newx = x, type = "response")
lasso_test_error <- mean((bike_data_train$bike_counts - lasso_predicted)^2)
lasso_test_error
ridge_predicted <- predict(cv_ridge_fit, s = cv_ridge_fit$lambda.min, newx = x, type = "response")
ridge_test_error <- mean((bike_data_train$bike_counts - ridge_predicted)^2)
ridge_test_error
rm(list=ls())
library(shiny); runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
ModelData
runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
str(df2)
runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
rm(list=ls())
library(shiny); runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
rm(list=ls())
rm(list=ls())
library(shiny); runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
rm(list=ls())
rm(list=ls())
library(shiny); runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
runApp('GitHub/Bacillus-cereus-exposure-assessment-model/BRisk app/BRisk.R')
rm(list=ls())
m(list=ls())
rm(list=ls())
rm(list=ls())
load("~/GitHub/MC_MilkSpoilageModel_Combined/Combined_model.RData")
# Determine percent spoiled
model_data$t_H = rep(1:end_day, times = n_sim * n_units)
filtered_counts <- map(1:end_day, ~ filter(model_data, t_H == .x) %>% pull(count_H))
percentages <- map(filtered_counts, ~ sum(. > log10(20000)) / length(.))
library(purrr)
filtered_counts <- map(1:end_day, ~ filter(model_data, t_H == .x) %>% pull(count_H))
library(tidyverse)
filtered_counts <- map(1:end_day, ~ filter(model_data, t_H == .x) %>% pull(count_H))
percentages <- map(filtered_counts, ~ sum(. > log10(20000)) / length(.))
percent_spoil <- data.frame(day = 1:end_day, percentage = unlist(percentages))
percent_spoil$percentage = percent_spoil$percentage * 100
percent_spoil
save.image("C:/Users/sujun/Documents/GitHub/MC_MilkSpoilageModel_Combined/Combined_model.RData")
library(shiny); runApp('GitHub/MC_MilkSpoilageModel_Combined/Combined Spoilage Model App.R')
View(model_data)
model_data$newLag_F <- lagAtNewTemp(model_data$T_F, growth_parameters$lag[model_data$allele_index], T0=growth_parameters$T0[model_data$allele_index])
model_data$newMu_F <- muAtNewTemp(model_data$T_F,growth_parameters$mumax[model_data$allele_index],T0=growth_parameters$T0[model_data$allele_index])
# Determine count_F
model_data = model_data %>%
rowwise() %>%
mutate(count_F = log10N(log10_init_mL, log10Nmax, newLag_F, newMu_F, t_F, model_type))
# Stage 2: Transport from facility to retail store
# Determine NewLag_T and NewMu_T
model_data$newLag_T <- lagAtNewTemp(model_data$T_T, growth_parameters$lag[model_data$allele_index], T0=growth_parameters$T0[model_data$allele_index])
model_data$newMu_T <- muAtNewTemp(model_data$T_T,growth_parameters$mumax[model_data$allele_index],T0=growth_parameters$T0[model_data$allele_index])
# Determine count_T
model_data = model_data %>%
rowwise() %>%
mutate(Lag_T = max(0, 1 - t_F/newLag_F) * newLag_T) %>%
mutate(Mu_T = if_else(T_T >= T_F * 0.75 & T_T <= T_F * 1.25, newMu_F, newMu_T)) %>%
mutate(count_T = log10N(count_F, log10Nmax, Lag_T, Mu_T, t_T, model_type))
# Stage 3: Display at retail
# Determine NewLag_S and NewMu_S
model_data$newLag_S <- lagAtNewTemp(model_data$T_S, growth_parameters$lag[model_data$allele_index], T0=growth_parameters$T0[model_data$allele_index])
model_data$newMu_S <- muAtNewTemp(model_data$T_S,growth_parameters$mumax[model_data$allele_index],T0=growth_parameters$T0[model_data$allele_index])
# Determine count_S
model_data = model_data %>%
rowwise() %>%
mutate(Lag_S = max(0, 1 - t_T/Lag_T) * newLag_S) %>%
mutate(Mu_S = if_else(T_S >= T_T * 0.75 & T_S <= T_T * 1.25, newMu_T, newMu_S)) %>%
mutate(count_S = log10N(count_T, log10Nmax, Lag_S, Mu_S, t_S, model_type))
# Stage 4: Transport from retail store to consumer home
# Determine NewLag_T2 and NewMu_T2
model_data$newLag_T2 <- lagAtNewTemp(model_data$T_T2, growth_parameters$lag[model_data$allele_index], T0=growth_parameters$T0[model_data$allele_index])
model_data$newMu_T2 <- muAtNewTemp(model_data$T_T2,growth_parameters$mumax[model_data$allele_index],T0=growth_parameters$T0[model_data$allele_index])
# Determine count_T2
model_data = model_data %>%
rowwise() %>%
mutate(Lag_T2 = max(0, 1 - t_S/Lag_S) * newLag_T2) %>%
mutate(Mu_T2 = if_else(T_T2 >= T_S * 0.75 & T_T2 <= T_S * 1.25, newMu_S, newMu_T2)) %>%
mutate(count_T2 = log10N(count_S, log10Nmax, Lag_T2, Mu_T2, t_T2, model_type))
# Stage 5: Storage at homes
# Determine NewLag_H and NewMu_H
model_data$newLag_H <- lagAtNewTemp(model_data$T_H, growth_parameters$lag[model_data$allele_index], T0=growth_parameters$T0[model_data$allele_index])
model_data$newMu_H <- muAtNewTemp(model_data$T_H,growth_parameters$mumax[model_data$allele_index],T0=growth_parameters$T0[model_data$allele_index])
# Determine count_H
end_day = input$day
model_data = model_data %>%
slice(rep(1:n(), each = end_day))
end_day = 35
model_data = model_data %>%
slice(rep(1:n(), each = end_day))
model_data = model_data %>%
rowwise() %>%
mutate(Lag_H = max(0, 1 - t_T2/Lag_T2) * newLag_H) %>%
mutate(Mu_H = if_else(T_H >= T_T2 * 0.75 & T_H <= T_T2 * 1.25, newMu_T2, newMu_H)) %>%
mutate(count_H = log10N(count_T2, log10Nmax, Lag_H, Mu_H, t_H, model_type))
View(moddel_data)
view(model_data)
rm(list=ls())
library(shiny); runApp('GitHub/MC_MilkSpoilageModel_Combined/Combined Spoilage Model App.R')
runApp('GitHub/MC_MilkSpoilageModel_Combined/Combined Spoilage Model App.R')
rm(list=ls())
knitr::opts_knit$set(root.dir = "C:/Users/sujun/Documents/GitHub/Bacillus-cereus-exposure-assessment-model")
# load packages
library(nlsMicrobio)
library(minpack.lm)
# load primary growth models
source("Primary model/UtilityFunctions_baranyi.R")
# 22dC growth data
data1 <-read.csv("Primary model/InputFiles/data_22_new.csv")
# create a list of data frames, one for each isolate and rep
data_list <- split(data1, list(data1$Isolate, data1$Rep))
# 22dC starting values
starting_values_22dC <-read.csv("Primary model/InputFiles/Starting values_22dC.csv")
# initialize a list to store the fits
fit_list <- list()
# loop over each sample and its repetitions
for (i in 1:length(data_list)) {
# subset the data for the current sample and rep
sub_data <- data_list[[i]]
# extract the sample and rep information
isolate <- unique(sub_data$Isolate)
rep <- unique(sub_data$Rep)
# set the starting values for the current isolate and rep
start_values <- c(LOG10N0 = starting_values_22dC$LOG10N0[i],
lag = starting_values_22dC$lag[i],
mumax = starting_values_22dC$mumax[i],
LOG10Nmax = starting_values_22dC$LOG10Nmax[i])
# fit the Baranyi model to the subset of data
fit <- nlsLM(LOG10N ~ baranyi_log10N(t,lag,mumax,LOG10N0,LOG10Nmax), data=sub_data,
start = start_values,
lower = c(0,0,0,0))
# extract the model coefficients
coef_values <- coefficients(fit)
# add the fit and its summary to the fit list
fit_list[[i]] <- data.frame(isolate=isolate, rep=rep,
LOG10N0 = coef_values["LOG10N0"],
lag = coef_values["lag"],
mumax = coef_values["mumax"],
LOG10Nmax = coef_values["LOG10Nmax"])
}
fit
fit$resid()
fit$m$resid()
length(fit)
rm(list=ls())
knitr::opts_knit$set(root.dir = "C:/Users/sujun/Documents/GitHub/Bacillus-cereus-exposure-assessment-model")
# load packages
library(tibble)
library(dplyr)
library(tidyr)
library(formula.tools)    # to load 'rhs'
library(purrr)            # to load 'map'
library(deSolve)          # to load 'ode'
library(rlang)
# load utility functions
source("Exposure assessment model/UtilityFunctions_dynamic_growth.R")
data_Q0 = read.csv("Exposure assessment model/InputFiles/Q0_h0_summary.csv")
data_Nmax = read.csv("Exposure assessment model/InputFiles/Nmax_new.csv")
data_sec_model = read.csv("Exposure assessment model/InputFiles/sec_model_new.csv")
Clade = c("I","II","VII","IV","IV","IV","IV","II","III","IV","II","VII","II","V","V","IV")
simulation_input <- data.frame(isolate = data_Q0$isolate,
Q0 = data_Q0$Q0,
Nmax = data_Nmax$average_Nmax,
b = data_sec_model$Slope,
Tmin = data_sec_model$Tmin,
Clade)
simulation_input$Topt = sapply(simulation_input$Clade, xopt_func)
simulation_input$mu_opt = (simulation_input$b*(simulation_input$Topt-simulation_input$Tmin))^2
# define the env_cond_time vectors for each simulation
env_cond_times <- list(c(0:14), c(0:21), c(0:35), c(0:60), c(0:90))
# create a vector of isolate names
isolate_names <- unique(simulation_input$isolate)
# create a list to store the final counts
final_counts <- list()
# loop over each isolate
for (isolate_name in isolate_names) {
# get the simulation input for the current isolate
isolate_input <- simulation_input[simulation_input$isolate == isolate_name, ]
# loop over each env_cond_time vector
for (i in seq_along(env_cond_times)) {
env_cond_time <- env_cond_times[[i]]
# set the environment condition temperature based on the isolate
if (isolate_input$isolate == 125){
env_cond_temp <- 10
} else if (isolate_input$isolate == 638){
env_cond_temp <- 6
} else {
env_cond_temp <- 8
}
# run simulation
my_primary <- list(mu_opt = isolate_input$mu_opt, Nmax = isolate_input$Nmax, N0 = 1e2, Q0 = isolate_input$Q0)
sec_temperature <- list(model = "reducedRatkowsky", xmin = isolate_input$Tmin, b = isolate_input$b, clade = isolate_input$Clade)
my_secondary <- list(temperature = sec_temperature)
growth <- predict_dynamic_growth(
times = env_cond_time,
env_conditions = tibble(time = env_cond_time, temperature = rep(env_cond_temp, length(env_cond_time))),
my_primary,
my_secondary
)
sim <- growth$simulation
final_count <- tail(sim$logN, 1)[[1]]
# store the final count for the isolate and env_cond_time combination
final_counts[[paste(isolate_name, "_", i, sep = "")]] <- final_count
}
}
# create a data frame of isolate names, final counts and days
count_by_isolate <- data.frame(isolate_env_time = names(final_counts), count = unlist(final_counts))
days = rep(c(14,21,35,60,90),16)
count_by_isolate$days = days
write.csv(count_by_isolate,"Exposure assessment model/OutputFiles/simulation_result.csv")
# define the env_cond_time vectors for each simulation
env_cond_times <- list(c(0:14), c(0:21), c(0:35), c(0:60), c(0:90))
# create a vector of isolate names
isolate_names <- unique(simulation_input$isolate)
# create a list to store the final counts
final_counts <- list()
# loop over each isolate
for (isolate_name in isolate_names) {
# get the simulation input for the current isolate
isolate_input <- simulation_input[simulation_input$isolate == isolate_name, ]
# loop over each env_cond_time vector
for (i in seq_along(env_cond_times)) {
env_cond_time <- env_cond_times[[i]]
# set the environment condition temperature based on the isolate
if (isolate_input$isolate == 125){
env_cond_temp <- 12
} else if (isolate_input$isolate == 638){
env_cond_temp <- 7
} else {
env_cond_temp <- 9
}
# run simulation
my_primary <- list(mu_opt = isolate_input$mu_opt, Nmax = isolate_input$Nmax, N0 = 1e2, Q0 = isolate_input$Q0)
sec_temperature <- list(model = "reducedRatkowsky", xmin = isolate_input$Tmin, b = isolate_input$b, clade = isolate_input$Clade)
my_secondary <- list(temperature = sec_temperature)
growth <- predict_dynamic_growth(
times = env_cond_time,
env_conditions = tibble(time = env_cond_time, temperature = rep(env_cond_temp, length(env_cond_time))),
my_primary,
my_secondary
)
sim <- growth$simulation
final_count <- tail(sim$logN, 1)[[1]]
# store the final count for the isolate and env_cond_time combination
final_counts[[paste(isolate_name, "_", i, sep = "")]] <- final_count
}
}
# create a data frame of isolate names, final counts and days
count_by_isolate <- data.frame(isolate_env_time = names(final_counts), count = unlist(final_counts))
days = rep(c(14,21,35,60,90),16)
count_by_isolate$days = days
#write.csv(count_by_isolate,"Exposure assessment model/OutputFiles/simulation_result.csv")
count_by_isolate
View(count_by_isolate)
isolate = "125_1"
cleaned_isolate = isolate.split("_")[0]
print(cleaned_isolate)
count_by_isolate$isolate <- gsub("_\\d+", "", count_by_isolate$isolate)
count_by_isolate$isolate
count_by_isolate
count_by_isolate <- count_by_isolate[, -1]
count_by_isolate
# define the env_cond_time vectors for each simulation
env_cond_times <- list(c(0:14), c(0:21), c(0:35), c(0:60), c(0:90))
# create a vector of isolate names
isolate_names <- unique(simulation_input$isolate)
# create a list to store the final counts
final_counts <- list()
# loop over each isolate
for (isolate_name in isolate_names) {
# get the simulation input for the current isolate
isolate_input <- simulation_input[simulation_input$isolate == isolate_name, ]
# loop over each env_cond_time vector
for (i in seq_along(env_cond_times)) {
env_cond_time <- env_cond_times[[i]]
# set the environment condition temperature based on the isolate
if (isolate_input$isolate == 125){
env_cond_temp <- 10
} else if (isolate_input$isolate == 638){
env_cond_temp <- 6
} else {
env_cond_temp <- 8
}
# run simulation
my_primary <- list(mu_opt = isolate_input$mu_opt, Nmax = isolate_input$Nmax, N0 = 1e2, Q0 = isolate_input$Q0)
sec_temperature <- list(model = "reducedRatkowsky", xmin = isolate_input$Tmin, b = isolate_input$b, clade = isolate_input$Clade)
my_secondary <- list(temperature = sec_temperature)
growth <- predict_dynamic_growth(
times = env_cond_time,
env_conditions = tibble(time = env_cond_time, temperature = rep(env_cond_temp, length(env_cond_time))),
my_primary,
my_secondary
)
sim <- growth$simulation
final_count <- tail(sim$logN, 1)[[1]]
# store the final count for the isolate and env_cond_time combination
final_counts[[paste(isolate_name, "_", i, sep = "")]] <- final_count
}
}
# create a data frame of isolate names, final counts and days
count_by_isolate <- data.frame(isolate_env_time = names(final_counts), count = unlist(final_counts))
days = rep(c(14,21,35,60,90),16)
count_by_isolate$days = days
write.csv(count_by_isolate,"Exposure assessment model/OutputFiles/simulation_result.csv")
count_by_isolate
count_by_isolate$isolate <- gsub("_\\d+", "", count_by_isolate$isolate)
count_by_isolate <- count_by_isolate[, -1]
count_by_isolate
new_row <- data.frame(count = 2, days = 0, isolate = unique(count_by_isolate$isolate))
new_row
count_by_isolate = rbind(count_by_isolate,new_row)
count_by_isolate
ggplot(count_by_isolate, aes(x = days, y = count)) +
geom_point() +
geom_line() +
facet_wrap(~ isolate, ncol = 1) +
labs(x = "Days", y = "Count") +
theme_minimal()
library(ggplot2)
ggplot(count_by_isolate, aes(x = days, y = count)) +
geom_point() +
geom_line() +
facet_wrap(~ isolate, ncol = 1) +
labs(x = "Days", y = "Count") +
theme_minimal()
ggplot(data, aes(x = days, y = count)) +
geom_point() +
facet_wrap(~ isolate, ncol = 1) +
labs(x = "Days", y = "Count") +
theme_minimal()
ggplot(count_by_isolate, aes(x = days, y = count)) +
geom_point() +
facet_wrap(~ isolate, ncol = 1) +
labs(x = "Days", y = "Count") +
theme_minimal()
plot(count_by_isolate$days,count_by_isolate$count)
library(ggplot2)
ggplot(count_by_isolate, aes(x = days, y = count)) +
geom_point() +
facet_wrap(~ isolate, ncol = 1) +
labs(x = "Days", y = "Count") +
theme_minimal()
library(ggplot2)
ggplot(count_by_isolate, aes(x = days, y = count, color = as.factor(isolate))) +
geom_line() +
geom_point() +
labs(x = "Days", y = "Count") +
scale_color_discrete(name = "Isolate") +
theme_minimal()
ggplot(count_by_isolate, aes(x = days, y = count)) +
geom_point() +
geom_line() +
facet_wrap(~ isolate, ncol = 1) +
labs(x = "Storae Days", y = "B cereus Count (log10 CFU/mL)") +
theme_minimal()
ggplot(count_by_isolate, aes(x = days, y = count, color = as.factor(isolate))) +
geom_line() +
geom_point() +
labs(x = "Storage Days", y = "B cereus Count (log10 CFU/mL)") +
scale_color_discrete(name = "Isolate") +
theme_minimal()
ggplot(count_by_isolate, aes(x = days, y = count, color = as.factor(isolate))) +
geom_line() +
geom_point() +
labs(x = "Storage Days", y = "B cereus Count (log10 CFU/mL)") +
scale_x_continuous(breaks = c(0, 14, 21, 35, 60, 90)) +
scale_color_discrete(name = "Isolate") +
theme_minimal()
rm(list=ls())
library(shiny); runApp('GitHub/Combined-Spoilage-Model-App/Combined Spoilage Model App.R')
runApp('GitHub/Combined-Spoilage-Model-App/Combined Spoilage Model App.R')
runApp('GitHub/Combined-Spoilage-Model-App/Combined Spoilage Model App.R')
library(shiny); runApp('Combined Spoilage Model App.R')
library(shiny); runApp('Combined Spoilage Model App.R')
library(shiny); runApp('Combined Spoilage Model App.R')
library(shiny); runApp('Combined Spoilage Model App.R')
library(shiny); runApp('Combined Spoilage Model App.R')
runApp('~/GitHub/PPC-Model-App/App_PPC_new.R')
runApp('~/GitHub/PPC-Model-App/App_PPC_new.R')
rm(list=ls())
runApp('Combined Spoilage Model App.R')
runApp('Combined Spoilage Model App.R')
runApp('Combined Spoilage Model App.R')
library(shiny); runApp('Combined Spoilage Model App.R')
library(shiny); runApp('Combined Spoilage Model App.R')
runApp('Combined Spoilage Model App.R')
library(shiny); runApp('Combined Spoilage Model App.R')
